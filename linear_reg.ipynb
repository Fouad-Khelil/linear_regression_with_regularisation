{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Implementation\n",
    "\n",
    "This code implements  a simple linear regression model. The class contains methods for model fitting (`fit`), prediction (`predict`), and evaluation of mean squared error (`mean_squared_error`). Additionally, it supports regularization techniques such as Lasso and Ridge.\n",
    "\n",
    "## Attributes\n",
    "\n",
    "- `learning_rate`: The learning rate for gradient descent optimization.\n",
    "- `n_iterations`: The number of iterations for gradient descent.\n",
    "- `weights`: Coefficients for each feature.\n",
    "- `bias`: Bias term.\n",
    "- `history`: List to store the cost history during training.\n",
    "- `threshold`: Convergence threshold.\n",
    "- `lambdaa`: Regularization parameter.\n",
    "- `reg`: Regularization type, can be either 'Lasso' or 'Ridge'.\n",
    "\n",
    "## Methods\n",
    "\n",
    "- `mean_squared_error(y, y_pred)`: Calculates the mean squared error between actual and predicted values.\n",
    "- `reg_mean_squared_error(y, y_pred)`: Calculates regularized mean squared error with Ridge regularization.\n",
    "- `regLasso_mean_squared_error(y, y_pred)`: Calculates regularized mean squared error with Lasso regularization.\n",
    "- `show_cost_history()`: Prints the cost history during training.\n",
    "- `last_history()`: Prints the last recorded cost in the history.\n",
    "- `fit(X, y)`: Fits the linear regression model to the training data.\n",
    "- `predict(X)`: Predicts the target variable for new data points.\n",
    "\n",
    "This implementation allows users to choose between Lasso, Ridge regularization, or no regularization. It also provides functionality to track the cost history during training and convergence monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pprint as p\n",
    "\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=100,reg='Lasso' , lambdaa= 1):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.history = []\n",
    "        self.threshold = 0.001\n",
    "        self.lambdaa=lambdaa\n",
    "        self.reg=reg\n",
    "    def mean_squared_error(self, y,  y_pred):\n",
    "        return np.mean(np.square(y - y_pred))    \n",
    "    \n",
    "    \n",
    "    def reg_mean_squared_error(self,y,y_pred):\n",
    "        return 0.5*np.mean(np.square(y - y_pred)) + 0.5*self.lambdaa*np.sum(np.square(self.weights))\n",
    "    \n",
    "    def regLasso_mean_squared_error(self,y,y_pred):\n",
    "        return 0.5*np.mean(np.square(y - y_pred) ) + 0.5*self.lambdaa*np.sum(np.abs(self.weights))\n",
    "    \n",
    "    def show_cost_history(self) : \n",
    "        print(\"the cost history is : \\n\")\n",
    "        p.pprint(self.history)\n",
    "        print(\"-\"*30)\n",
    "    \n",
    "    def last_history(self):\n",
    "        print(\"the cost history is : \\n\")\n",
    "        p.pprint(self.history[-1])\n",
    "        print(\"-\"*30) \n",
    "              \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            y_predicted = np.dot(X, self.weights) + self.bias\n",
    "            if self.reg=='Ridge':\n",
    "                dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y)) + self.lambdaa*self.weights\n",
    "            elif self.reg == 'Lasso' : \n",
    "                dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y)) + self.lambdaa * np.sign(self.weights)\n",
    "\n",
    "            else :\n",
    "                dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y)) \n",
    "\n",
    "            # Compute gradient\n",
    "    \n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "\n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "            \n",
    "             # Compute and store cost\n",
    "            #cost = self.mean_squared_error(y, y_predicted)\n",
    "            \n",
    "            if self.reg=='Ridge':\n",
    "                cost=self.reg_mean_squared_error(y, y_predicted)\n",
    "            elif self.reg == 'Lasso' : \n",
    "                cost=self.regLasso_mean_squared_error(y, y_predicted)\n",
    "            else :\n",
    "                cost=self.mean_squared_error(y, y_predicted)\n",
    "\n",
    "            \n",
    "            cost=self.reg_mean_squared_error(y, y_predicted)\n",
    "            if(self.history != []) :\n",
    "                last_cost = self.history[-1]\n",
    "            \n",
    "                if abs(last_cost - cost) < self.threshold:\n",
    "                    print(f\"Converged after {i+1} iterations.\")\n",
    "                    break\n",
    "            \n",
    "            self.history.append(cost)\n",
    "\n",
    "    def predict(self, X):\n",
    "        print(\"weights are :\",self.weights)\n",
    "        print(\"biases are :\",self.bias)\n",
    "\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using  : \n",
    "- Fake dataset \n",
    "- Ridge \n",
    "- lamda =1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 10 iterations.\n",
      "the cost history is : \n",
      "\n",
      "0.48244382541663594\n",
      "------------------------------\n",
      "weights are : [0.21465106 0.29525744 0.37586382]\n",
      "biases are : 0.08676669072062641\n",
      "Predictions: [5.5626133  6.44838561]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sample dataset\n",
    "X = np.array([[1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7]])\n",
    "y = np.array([2, 4, 5, 4, 5])\n",
    "\n",
    "# Training\n",
    "model = LinearRegression(learning_rate=0.01, n_iterations=1000, reg='Ridge' , lambdaa=1)\n",
    "model.fit(X, y)\n",
    "model.last_history()\n",
    "\n",
    "# Prediction\n",
    "X_test = np.array([[5, 6, 7], [6, 7, 8]])\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using \n",
    "- fake dataset \n",
    "- Ridge \n",
    "- lamda =10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 352 iterations.\n",
      "the cost history is : \n",
      "\n",
      "0.6770080528383096\n",
      "------------------------------\n",
      "weights are : [0.12239739 0.15447446 0.18655153]\n",
      "biases are : 1.7689299532350942\n",
      "Predictions: [4.61362432 5.0770477 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training\n",
    "model = LinearRegression(learning_rate=0.01, n_iterations=1000, reg='Ridge', lambdaa=10)\n",
    "model.fit(X, y)\n",
    "model.last_history()\n",
    "\n",
    "# Prediction\n",
    "X_test = np.array([[5, 6, 7], [6, 7, 8]])\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Predictions:\", y_pred) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using \n",
    "- iris dataset \n",
    "- without regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 34 iterations.\n",
      "the cost history is : \n",
      "\n",
      "[0.8246339742361111,\n",
      " 0.2933528559041411,\n",
      " 0.21056371035264362,\n",
      " 0.1933777249971514,\n",
      " 0.18586763872710993,\n",
      " 0.1799530124456717,\n",
      " 0.17449422665174072,\n",
      " 0.1693352688972842,\n",
      " 0.16445073696330997,\n",
      " 0.15982933673292313,\n",
      " 0.15546018219872365,\n",
      " 0.15133208796470177,\n",
      " 0.14743395518854915,\n",
      " 0.14375500485703435,\n",
      " 0.14028486873431745,\n",
      " 0.13701361383227628,\n",
      " 0.13393174007474415,\n",
      " 0.13103016791818548,\n",
      " 0.12830022257049994,\n",
      " 0.12573361737019068,\n",
      " 0.12332243729433028,\n",
      " 0.12105912294859056,\n",
      " 0.11893645515665603,\n",
      " 0.11694754017646469,\n",
      " 0.11508579553695186,\n",
      " 0.11334493647672256,\n",
      " 0.11171896296206123,\n",
      " 0.11020214726079863,\n",
      " 0.10878902204884311,\n",
      " 0.10747436902689264,\n",
      " 0.10625320802569324,\n",
      " 0.10512078657908441,\n",
      " 0.10407256994493702]\n",
      "------------------------------\n",
      "weights are : [ 0.0461052  -0.05330663  0.22037995  0.10096835]\n",
      "biases are : -0.006460576620232209\n",
      "Predictions: [1.28247038 0.4587103  1.9628011  1.25874371 1.35697568 0.43222223\n",
      " 1.021766   1.50257971 1.30527939 1.09766552 1.44851646 0.37355326\n",
      " 0.36723243 0.39487112 0.38697119 1.30542544 1.63363743 1.08900897\n",
      " 1.23004914 1.59561224 0.41245424 1.37646605 0.43581815 1.58551541\n",
      " 1.7675737  1.52072733 1.62912444 1.66894249 0.39374693 0.42239543]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the Iris dataset\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data, iris.target\n",
    "\n",
    "    # Use only one feature for simplicity\n",
    "    #X = X[:, :1]  # Using only the first feature\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Training\n",
    "    model = LinearRegression(learning_rate=0.01, n_iterations=1000, reg='noreg')\n",
    "    model.fit(X_train, y_train)\n",
    "    model.show_cost_history()\n",
    "\n",
    "    # Prediction\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using : \n",
    "- Iris dataset \n",
    "- With regularisation (Ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged after 34 iterations.\n",
      "the cost history is : \n",
      "\n",
      "[0.8246339742361111,\n",
      " 0.29324757028192544,\n",
      " 0.2115635170747342,\n",
      " 0.19413909981638725,\n",
      " 0.1864287646988694,\n",
      " 0.18053630639733936,\n",
      " 0.17524364455009767,\n",
      " 0.1703269363254957,\n",
      " 0.16572485706945717,\n",
      " 0.1614082387264408,\n",
      " 0.15735651583214552,\n",
      " 0.15355240099711287,\n",
      " 0.14998034718968292,\n",
      " 0.1466260049482915,\n",
      " 0.14347599474672745,\n",
      " 0.14051779105802567,\n",
      " 0.13773964901064656,\n",
      " 0.13513054866489532,\n",
      " 0.13268014759572544,\n",
      " 0.1303787382114151,\n",
      " 0.128217208375295,\n",
      " 0.1261870047009475,\n",
      " 0.12428009819713057,\n",
      " 0.12248895205842726,\n",
      " 0.12080649144749335,\n",
      " 0.11922607513820394,\n",
      " 0.11774146890225251,\n",
      " 0.11634682053092392,\n",
      " 0.11503663639114549,\n",
      " 0.11380575942139462,\n",
      " 0.1126493484789427,\n",
      " 0.11156285895539561,\n",
      " 0.11054202458260488]\n",
      "------------------------------\n",
      "weights are : [ 0.05226068 -0.04003198  0.19933743  0.09012546]\n",
      "biases are : -0.002010176302065689\n",
      "Predictions: [1.24972691 0.50966546 1.87903074 1.22766779 1.32426822 0.47914511\n",
      " 1.00933474 1.45839884 1.26614232 1.07858196 1.40645373 0.41683011\n",
      " 0.4224754  0.43798672 0.43844157 1.27621324 1.57202141 1.06712367\n",
      " 1.19796769 1.53493425 0.45247768 1.33566327 0.47817458 1.52592171\n",
      " 1.71473815 1.47188364 1.56643936 1.60863951 0.4348552  0.46170694]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training\n",
    "model = LinearRegression(learning_rate=0.01, n_iterations=1000, reg='Ridge')\n",
    "model.fit(X_train, y_train)\n",
    "model.show_cost_history()\n",
    "\n",
    "# Prediction\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
